defaults:

  # Trainer
  logdir: /dev/null
  run: train_fixed_eval
  seed: 0
  task: dummy_discrete
  env: {amount: 4, parallel: process, daemon: False, repeat: 1, size: [64, 64], camera: -1, gray: False, length: 0, discretize: 0, lives: False, sticky: True, episodic: True, restart: True, resets: True, seed: 0, kbreset: False}
  replay: fixed
  replay_size: 1e6
  replay_chunk: 64
  replay_fixed: {length: 0, prio_starts: 0.0, prio_ends: 1.0, sync: 0, minlen: 0}
  replay_consec: {randomize: False, sync: 0}
  replay_prio: {prio_starts: 0.0, prio_ends: 1.0, sync: 0, fraction: 0.1, softmax: False, temp: 1.0, constant: 0.0, exponent: 0.5}
  tf: {jit: True, platform: gpu, precision: float16, debug_nans: False, logical_gpus: 0, dist_dataset: False, dist_policy: False, tensorfloat: True, soft_placement: False, growth: True}
  eval_dir: ''
  filter: '.*'
  tbtt: 0
  train:
    steps: 1e10
    expl_until: 0
    log_every: 1e4
    eval_every: 1e5
    eval_eps: 1
    eval_samples: 1
    train_every: 16
    train_steps: 1
    train_fill: 1e4
    eval_fill: 1e4
    pretrain: 1
    log_zeros: False
    log_keys_video: [image]
    log_keys_sum: '^$'
    log_keys_mean: '^$'
    log_keys_max: '^$'
    sync_every: 180

  # Agent
  task_behavior: Greedy
  expl_behavior: None
  transform_rewards: off
  batch_size: 32
  data_loader: tfdata
  expl_noise: 0.0
  eval_noise: 0.0
  priority: reward_loss
  priority_correct: 0.0
  train_wm: True
  known_reward: none

  # World Model
  grad_heads: [decoder, reward, cont]
  rssm: {units: 1024, deter: 1024, stoch: 32, classes: 32, act: elu, norm: layer, initial: learned2, unimix: 0.01, prior_layers: 3, post_layers: 1, gru_layers: 1, unroll: True}
  encoder: {mlp_keys: '.*', cnn_keys: '.*', act: elu, norm: layer, mlp_layers: 4, mlp_units: 512, cnn: simple, cnn_depth: 64, cnn_kernels: [4, 4, 4, 4], cnn_blocks: 2}
  decoder: {mlp_keys: '.*', cnn_keys: '.*', act: elu, norm: layer, mlp_layers: 4, mlp_units: 512, cnn: simple, cnn_depth: 64, cnn_kernels: [5, 5, 6, 6], cnn_blocks: 2, image_dist: mse, inputs: [deter, stoch]}
  reward_head: {layers: 4, units: 512, act: elu, norm: layer, dist: symlog, outscale: 0.1, inputs: [deter, stoch]}
  cont_head: {layers: 4, units: 512, act: elu, norm: layer, dist: binary, outscale: 0.1, inputs: [deter, stoch]}
  loss_scales: {kl: 1.0, image: 1.0, reward: 1.0, cont: 1.0}
  model_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2, wd_pattern: 'kernel', warmup: 0}
  wmkl: {impl: mult, scale: 0.1, target: 3.5, min: 1e-3, max: 1.0, vel: 0.1}
  wmkl_balance: 0.8

  # Actor Critic
  actor: {layers: 4, units: 512, act: elu, norm: layer, minstd: 0.03, maxstd: 1.0, outscale: 0.1, unimix: 0.01, inputs: [deter, stoch]}
  critic: {layers: 4, units: 512, act: elu, norm: layer, dist: symlog, outscale: 0.1, inputs: [deter, stoch]}
  actor_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2, wd_pattern: 'kernel', warmup: 0}
  critic_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2, wd_pattern: 'kernel', warmup: 0}
  actor_dist_disc: onehot
  actor_dist_cont: normal
  discount: 0.998
  imag_horizon: 15
  imag_unroll: True
  return_lambda: 0.95
  actor_return: gve
  critic_return: gve
  actor_grad_disc: reinforce
  actor_grad_cont: backprop
  slow_target: True
  slow_target_update: 100
  slow_target_fraction: 1.0
  actent: {impl: mult, scale: 3e-3, target: 0.5, min: 1e-5, max: 1.0, vel: 0.1}
  actent_norm: True
  advnorm: {impl: mean_std, decay: 0.99, max: 1e8}
  retnorm: {impl: off, decay: 0.999, max: 1e8}
  scorenorm: {impl: off, decay: 0.99, max: 1e8}
  critic_type: vfunction
  # pengs_qlambda: False
  # hyper_discs: [0.9, 0.99, 0.999]

  # Exploration
  expl_rewards: {extr: 0.0, disag: 0.0, vae: 0.0, ctrl: 0.0, pbe: 0.0}
  expl_discount: 0.99
  expl_retnorm: {impl: std, decay: 0.999, max: 1e8}
  expl_scorenorm: {impl: off, decay: 0.999, max: 1e8}
  expl_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2, warmup: 0}
  disag_head: {layers: 4, units: 512, act: elu, norm: layer, dist: mse, outscale: 1.0, inputs: [deter, stoch, action]}
  disag_target: [stoch]
  disag_models: 8
  ctrl_embed: {layers: 3, units: 512, act: elu, norm: layer, dist: mse, inputs: [deter, stoch]}
  ctrl_head: {layers: 1, units: 128, act: elu, norm: layer, dist: mse, inputs: [current, next]}
  ctrl_size: 32
  ctrl_opt: {opt: adam, lr: 1e-4, eps: 1e-6, clip: 100.0, wd: 1e-2, warmup: 0}
  pbe_inputs: [deter]
  pbe_knn: 16
  expl_enc: {layers: 4, units: 512, act: elu, norm: layer, dist: onehot, outscale: 0.1, inputs: [deter], shape: [8, 8]}
  expl_dec: {layers: 4, units: 512, act: elu, norm: layer, dist: mse, outscale: 0.1}
  expl_kl: {impl: mult, scale: 0.1, target: 10.0, min: 1e-2, max: 1.0, vel: 0.1}
  expl_vae_elbo: False
  expl_when_buffer: 1e4
  expl_when_frac: 0.1
  expl_when_every: 50
  expl_when_random: False

qfunction:

  critic_type: qfunction
  discount: 0.99
  pengs_qlambda: False  # True
  return_lambda: 0.5

minecraft:

  task: minecraft_discover
  train.log_keys_max: '^log_inventory.*'
  env: {amount: 16, parallel: thread}
  encoder: {mlp_keys: 'inventory|equipped|reward|is_terminal', cnn_keys: 'image'}
  decoder: {mlp_keys: 'inventory|equipped', cnn_keys: 'image'}
  tf.precision: float32
  train.eval_fill: 1e5
  train.train_every: 64

dmlab:

  task: dmlab_rooms_collect_good_objects_train
  encoder: {mlp_keys: 'reward', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  env: {repeat: 4, episodic: False, amount: 16}

atari:

  task: atari_pong
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  env: {gray: True, repeat: 4, amount: 16}
  train.steps: 5e7
  train.eval_every: 2.5e5

atari100k:

  task: atari_pong
  run: train_eval
  train: {steps: 2e5, eval_every: 1e5, eval_eps: 10}
  tf.precision: float32
  env: {gray: True, repeat: 4}
  env.sticky: False
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  train.train_every: 1
  train.train_fill: 1000

crafter:

  task: crafter_reward
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  train.log_keys_max: '^log_achievement_.*'
  train.log_keys_sum: '^log_reward$'
  train.train_every: 8

dmc_vision:

  task: dmc_walker_walk
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}
  env.repeat: 2
  train.train_every: 2

dmc_proprio:

  task: dmc_walker_walk
  encoder: {mlp_keys: '.*', cnn_keys: '$^'}
  decoder: {mlp_keys: '.*', cnn_keys: '$^'}
  env.repeat: 2
  train.train_every: 2

robodesk:

  run: train_eval
  task: robodesk_push_slide_dense
  encoder: {mlp_keys: '^$', cnn_keys: 'image'}
  decoder: {mlp_keys: '^$', cnn_keys: 'image'}
  env: {parallel: process, amount: 8, repeat: 4, length: 2000}
  train: {eval_every: 2.5e4, eval_eps: 4, train_every: 4}
  discount: 0.99

loconav_ant:

  task: loconav_ant_umaze
  encoder: {mlp_keys: '.*', cnn_keys: 'image'}
  decoder: {mlp_keys: '.*', cnn_keys: 'image'}
  train.log_keys_max: '^log_.*'
  env.repeat: 2
  train.train_every: 2

loconav_quadruped:

  task: loconav_quadruped_umaze
  encoder: {mlp_keys: '.*(joints|act|acc|vel).*', cnn_keys: 'image'}
  decoder: {mlp_keys: '.*(joints|act|acc|vel).*', cnn_keys: 'image'}
  train.log_keys_max: '^log_.*'
  env.repeat: 2
  train.train_every: 2

hrlgrid:

  task: hrlgrid_16
  encoder: {mlp_keys: '$^', cnn_keys: 'image'}
  decoder: {mlp_keys: '$^', cnn_keys: 'image'}

a1:

   task: a1_sim
   run: train
   encoder: {mlp_keys: 'vector', cnn_keys: '$^'}
   decoder: {mlp_keys: 'vector', cnn_keys: '$^'}
   batch_size: 32  # 64
   replay_chunk: 32
   env.amount: 1
   # env.repeat: 25
   env.repeat: 50
   env.resets: False
   env.length: 250
   env.parallel: none
   env.restart: False
   train.train_every: 4
   actent.target: 0.5  # 0.3
   discount: 0.995
   rssm: {deter: 256, units: 256}
   # train.train_fill: 1000
   train.train_fill: 5000
   train.sync_every: 20
   actor.minstd: 0.1
   actor.unimix: 0.1

xarm:

   task: xarm_dummy
   run: train
   encoder: {mlp_keys: 'cartesian|joint|gripper|grasped', cnn_keys: 'image|depth'}
   decoder: {mlp_keys: 'cartesian|joint|gripper|grasped', cnn_keys: 'image|depth'}
   batch_size: 32
   replay_chunk: 32
   env.amount: 1
   env.repeat: 1
   env.parallel: none
   env.restart: False
   train.train_every: 4
   actent.target: 0.4
   discount: 0.995
   rssm: {deter: 512, units: 512}  # 1024
   train.log_every: 100
   train.eval_every: 20
   train.sync_every: 20
   actor.minstd: 0.1
   actor.unimix: 0.1
   train.log_keys_video: [image, depth]
   env.resets: True
   env.length: 100
   train.train_fill: 2000

ur5:

   task: ur5_dummy
   run: train
   encoder: {mlp_keys: 'cartesian|joint|gripper|grasped', cnn_keys: 'image'}
   decoder: {mlp_keys: 'cartesian|joint|gripper|grasped', cnn_keys: 'image'}
   batch_size: 32
   replay_chunk: 32
   env.amount: 1
   env.repeat: 1
   env.parallel: none
   env.restart: False
   train.train_every: 4
   actent.target: 0.4
   discount: 0.995
   rssm: {deter: 512, units: 512}  # 1024
   train.log_every: 100
   train.eval_every: 20
   train.sync_every: 20
   actor.minstd: 0.1
   actor.unimix: 0.1
   train.log_keys_video: [image, depth]
   env.resets: True
   env.length: 100
   train.train_fill: 2000

sphero:

   task: sphero_dummy
   run: train
   encoder: {mlp_keys: '$^', cnn_keys: 'image'}
   decoder: {mlp_keys: '$^', cnn_keys: 'image'}
   batch_size: 32
   replay_chunk: 32
   env.amount: 1
   env.repeat: 1
   env.parallel: none
   env.restart: True
   train.train_every: 4
   actent.target: 0.4
   discount: 0.995
   rssm: {deter: 512, units: 512}  # 1024
   train.log_every: 100
   train.eval_every: 20
   train.sync_every: 20
   actor.minstd: 0.1
   actor.unimix: 0.1
   train.log_keys_video: [image]
   train.log_keys_sum: '^log_.*'
   train.log_keys_mean: '^log_.*'
   train.log_zeros: True
   train.log_keys_max: '^log_.*'

   env.resets: True
   env.length: 100
   train.train_fill: 1000

plan2explore:

  expl_behavior: Explore
  expl_rewards.extr: 1.0
  expl_rewards.disag: 0.1
  # expl_retnorm.max: 1e2

small:

  env.amount: 2
  train:
    eval_every: 1e4
    log_every: 1e3
    train_fill: 100
    eval_fill: 100
    train_every: 16
  tf.precision: float32
  replay_chunk: 16
  rssm.deter: 128
  rssm.units: 128
  rssm.stoch: 8
  .*\.cnn_depth$: 16
  .*\.wd$: 0.0

debug:

  env.length: 100
  env.restart: False
  env.amount: 2
  train:
    eval_every: 300
    log_every: 300
    train_fill: 100
    eval_fill: 100
    train_steps: 1
    train_every: 30
  batch_size: 8
  replay_size: 500
  replay_chunk: 12
  encoder.cnn_depth: 16
  decoder.cnn_depth: 16
  rssm: {units: 64, stoch: 8, classes: 8}
  .*\.layers: 2
  .*\.units: 64
  .*\.wd: 0.0
  tf: {platform: gpu, jit: False}
